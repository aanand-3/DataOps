# -*- coding: utf-8 -*-
"""utilities.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zdfy-RtpK5C52ogsuZ5eXr_qJGWwfdWD
"""

# @title Import Packages
import re
import pandas as pd
import numpy as np
from functools import reduce
from pandas_gbq import to_gbq

from io import StringIO
from google.colab import auth, data_table, files

# @title Big Query Functions

# Function to execute a query and return the results as a DataFrame
def execute_query(client, query):
    """
    Execute a SQL query using Google BigQuery and return the results as a DataFrame.

    Args:
    query (str): The SQL query to be executed.
    client: A Google BigQuery client instance.

    Returns:
    DataFrame: The query results as a DataFrame.
    """
    query_job = client.query(query)
    df = query_job.to_dataframe()
    # Remove the index column
    df.reset_index(drop=True, inplace=True)
    return df

def df_to_bq(df, project, dataset, table):
    to_gbq(df, f'{dataset}.{table}', project_id=project, if_exists='replace')

# Create a BigQuery table from a DataFrame with auto-detected schema
def create_bigquery_table(df, project_id, dataset_id, table_id):
    """
    Create a BigQuery table from a Pandas DataFrame with auto-detected schema.

    Args:
        df (pd.DataFrame): The DataFrame to be loaded into BigQuery.
        project_id (str): The Google Cloud project ID.
        dataset_id (str): The BigQuery dataset ID.
        table_id (str): The name for the new BigQuery table.

    Returns:
        None: The function creates the table in BigQuery.

    Example:
        create_bigquery_table_auto_detect(df, 'your-project-id', 'your-dataset-id', 'your-table-id')
    """
    client = bigquery.Client(project=project_id)
    table_ref = client.dataset(dataset_id).table(table_id)
    job_config = bigquery.LoadJobConfig(
        autodetect=True  # Enable schema auto-detection
    )
    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
    job.result()
    print(f'Table {project_id}.{dataset_id}.{table_id} created from the DataFrame with auto-detected schema.')

def read_csv_from_gcs(gcs_bucket, file_path):
    """
    Read a CSV file from Google Cloud Storage (GCS) and load it into a Pandas DataFrame.

    Args:
        gcs_bucket (str): The name of the GCS bucket.
        file_path (str): The path to the CSV file in GCS.

    Returns:
        pd.DataFrame: A Pandas DataFrame containing the CSV data.
    """
    # Download the CSV file from GCS
    client = storage.Client()
    bucket = client.get_bucket(gcs_bucket)
    blob = storage.Blob(file_path, bucket)
    content = blob.download_as_text()

    # Create a DataFrame from the CSV data
    df = pd.read_csv(StringIO(content))

    return df

def bigquery_schema(df):
    """
    Generate a BigQuery schema based on the DataFrame's data types.

    Args:
        df (pandas.DataFrame): The DataFrame for which the schema is generated.

    Returns:
        list: A list of dictionaries representing the BigQuery schema with 'name' and 'type' keys.
    """
    dtype_mapping = {
        'object': 'STRING',
        'int64': 'INTEGER',
        'float64': 'FLOAT',
        'bool': 'BOOL',
        'datetime64[ns]': 'TIMESTAMP'
    }

    schema = [{'name': col_name, 'type': dtype_mapping.get(str(col_type), 'UNKNOWN')} for col_name, col_type in df.dtypes.items()]

    return schema

# Function to download a DataFrame as a CSV file
def download(df, fileName):
    """
    Download a DataFrame as a CSV file.

    Args:
    df (DataFrame): The input DataFrame to be downloaded.
    fileName (str): The name of the CSV file (without the .csv extension).

    Returns:
    None
    """
    if df is not None:
      fileName = f'{fileName}.csv'
      filepath = "/content/drive/MyDrive/Colab Downloads/"
      file = filepath + fileName
      df.to_csv(file, index=False)
      files.download(file)

# @title Pandas

def split_dataframe(df, index_column, mapping):

    df = df.set_index(index_column)
    # Initialize DataFrames for SF and DNB sources
    source1 = next(iter(mapping.keys())).split('_', 1)[0]
    source2 = next(iter(mapping.values())).split('_', 1)[0]
    
    data1 = {'GroupID': df.index, 'Source': source1}
    data2 = {'GroupID': df.index, 'Source': source2}

    for key, value in mapping.items():
        data1[key] = df[key]
        data2[key] = df[value]
    
    # Create DataFrames from the dictionaries
    df1 = pd.DataFrame(data1)
    df2 = pd.DataFrame(data2)
    
    # Concatenate the DataFrames
    df_split = pd.concat([df1, df2], ignore_index=True)
    df_split.sort_values(by='GroupID', inplace=True)
    
    return df_split

def swap_values(mapping):
    """
    Swaps the values in the lists within the given mapping dictionary.
    
    Parameters:
    mapping (dict): A dictionary where each key maps to a list of two values.
    
    Returns:
    dict: The updated dictionary with the values in the lists swapped.
    
    Example:
    original_mapping = {
        'key1': ['value1', 'value2'],
        'key2': ['value3', 'value4']
    }
    
    swapped_mapping = swap_values(original_mapping)
    # swapped_mapping will be:
    # {
    #     'key1': ['value2', 'value1'],
    #     'key2': ['value4', 'value3']
    # }
    """
    for key, value_list in mapping.items():
        if len(value_list) == 2:  # Ensure there are exactly two elements to swap
            value_list[0], value_list[1] = value_list[1], value_list[0]
    return mapping

def split_rows(df, index_column, mapping, num_datasets=2):
    """
      Splits a dataframe row into multiple rows based on a provided column mapping.

      Parameters:
      - df (pd.DataFrame): The input dataframe to be split.
      - index_column (str): The column name to be used as the index.
      - mapping (dict): A dictionary mapping primary keys to a list of corresponding keys for each dataset.
      - num_datasets (int): The number of datasets to split into (default is 2).

      Returns:
      - pd.DataFrame: A concatenated dataframe containing all the split datasets, sorted by the index column.
    """
    df = df.set_index(index_column)
    df_columns = list(df.columns)
    
    # Initialize data dictionaries for each source
    sources = [next(iter(mapping.keys())).split('_', 1)[0]] + [mapping[next(iter(mapping.keys()))][i].split('_', 1)[0] for i in range(num_datasets - 1)]
    data = [{'GroupID': df.index, 'Source': source} for source in sources]
    
    for key, value in mapping.items():
        # Assign values for the primary source
        data[0][key] = df[key] if key in df_columns else 0
        
        # Assign values for the additional sources
        for i in range(1, num_datasets):
            if value[i-1] in df_columns:
                data[i][key] = df[value[i-1]]
            else:
                data[i][key] = 0

    # Create DataFrames from the dictionaries
    dfs = [pd.DataFrame(d) for d in data]
    
    # Concatenate the DataFrames
    df_split = pd.concat(dfs, ignore_index=True)
    df_split.sort_values(by='GroupID', inplace=True)
    
    return df_split

def display_methods(obj, include=None, exclude=None):
    if include and exclude:
        methods = [method for method in dir(obj) if method.startswith(include) and not method.startswith(exclude)]
    elif include:
        methods = [method for method in dir(obj) if method.startswith(include)]
    elif exclude:
        methods = [method for method in dir(obj) if not method.startswith(exclude)]
    else:
        methods = [method for method in dir(obj)]

    df = create_df_from_list(methods)
    display(df)

def create_df_from_list(lst):
    # Calculate the necessary number of rows and columns
    num_items = len(lst)
    num_rows = 10
    num_columns = (num_items + num_rows - 1) // num_rows  # Ceiling division for column calculation

    # Ensure there are enough elements to reshape without error by padding the list if necessary
    padded_list = lst + [None] * (num_rows * num_columns - num_items)  # Pad list to match the exact reshape requirement

    # Reshape the list into a 2D array with num_rows rows and enough columns
    reshaped_data = np.array(padded_list).reshape((num_columns, num_rows)).T  # Transpose to meet the specified layout

    # Create a DataFrame from the reshaped data
    df = pd.DataFrame(reshaped_data)

    return df

def summary(df, text, columns):
    print(f'Dataframe Summary: {text}')
    print(f'\t Number of Columns: {df.shape[1]}')
    print(f'\t Number of Rows: {df.shape[0]}')
    for col in columns:
      print(f'\t Unique_{col}: {df[col].nunique()}')

def df_description(df, text, columns):
    print(f'Dataframe Summary: {text}')
    print(f'\t Number of Columns: {df.shape[1]}')
    print(f'\t Number of Rows: {df.shape[0]}')

    for col, op in columns.items():
        if op == 'mean':
            print(f'\t Mean of {col}: {df[col].mean()}')
        elif op == 'sum':
            print(f'\t Sum of {col}: {df[col].sum()}')
        elif op == 'count':
            print(f'\t Count of non-null values in {col}: {df[col].count()}')
        elif op == 'unique':
            print(f'\t Count of Unique values in {col}: {df[col].nunique()}')
        else:
            print(f'\t Unsupported operation "{op}" for column {col}')

def datatable(df):
    df = fillna_custom(df)
    return data_table.DataTable(df)

# Get value counts
def value_count(df, col):
    value_counts_df = df[col].value_counts().reset_index()
    value_counts_df.columns = [col, 'Count']
    return value_counts_df

def prefix_columns(df, prefix):
    """
    Prefixes each column name in the DataFrame with the specified prefix.

    Parameters:
    - df: pandas DataFrame
    - prefix: str, the prefix to be added to each column name

    Returns:
    - pandas DataFrame with prefixed column names
    """
    df = df.copy()
    df.columns = [prefix + col for col in df.columns]
    return df

def reorder_columns_by_prefix(df, prefix_order):
    """
    Reorders DataFrame columns based on a list of prefixes in the specified order.

    Parameters:
    - df: The DataFrame whose columns are to be reordered.
    - prefix_order: A list of string prefixes in the desired order.

    Returns:
    - A DataFrame with columns reordered according to the specified prefix order.
    """
    # Generate a sorted list of columns for each prefix
    sorted_columns = [sorted([col for col in df.columns if col.startswith(prefix)]) for prefix in prefix_order]

    # Flatten the list of sorted columns and concatenate any remaining columns that do not match the specified prefixes
    ordered_columns = sum(sorted_columns, []) + [col for col in df.columns if not any(col.startswith(prefix) for prefix in prefix_order)]

    # Reindex the DataFrame using the ordered list of columns
    return df.reindex(ordered_columns, axis=1)

def fillna_custom(df):
    # Fill NaN values with 0 for numeric columns
    numeric_cols = df.select_dtypes(include='number').columns
    df[numeric_cols] = df[numeric_cols].fillna(0)

    # Fill NaN values with an empty string for string columns
    string_cols = df.select_dtypes(include='object').columns
    df[string_cols] = df[string_cols].fillna('')

    return df

def convert_dtype(df, dtype_dict):
    """
    Convert the data type of columns in a DataFrame according to the specified dictionary.

    Parameters:
        df (DataFrame): The DataFrame to convert.
        dtype_dict (dict): A dictionary mapping target data types to lists of column names.

    Returns:
        DataFrame: The DataFrame with converted data types.

    Example:
        converted_df = convert_dtype(df, {
                                        'float': ['a', 'r'],
                                        'int': ['b', 'q'],
                                        'str': ['c'],
                                        'datetime64[ns]': ['date']
                                        })
    """
    converted_df = df.copy()
    for dtype, columns in dtype_dict.items():
        if dtype == 'int':
            # Convert columns to integer, handling non-integer values gracefully
            converted_df[columns] = converted_df[columns].apply(pd.to_numeric, errors='coerce').astype(dtype)
        else:
            # For other data types, simply convert the columns
            converted_df[columns] = converted_df[columns].astype(dtype)
        converted_df[columns] = converted_df[columns].astype(dtype)
    return converted_df

def convert_column_types(df, str_cols=[], int_cols=[], float_cols=[], category_cols=[], datetime_cols=[]):
    """
    Convert the data types of specified columns in a DataFrame.

    Args:
    df (pd.DataFrame): The DataFrame containing the columns to be converted.
    str_cols (list): A list of column names to be converted to string.
    int_cols (list): A list of column names to be converted to integer.
    float_cols (list): A list of column names to be converted to float.
    category_cols (list): A list of column names to be converted to category.
    datetime_cols (list): A list of column names to be converted to datetime.

    Returns:
    pd.DataFrame: The DataFrame with the specified columns converted to the desired data types.
    """

    # Define a dictionary mapping data types to their corresponding functions
    type_mapping = {
        'str': str,
        'int': int,
        'float': float,
        'category': 'category',
        'datetime': pd.to_datetime
    }

    # Iterate over the data types and their corresponding columns
    for data_type, cols in zip(['str', 'int', 'float', 'category', 'datetime'],
                               [str_cols, int_cols, float_cols, category_cols, datetime_cols]):
        for col in cols:
            if col not in df.columns:
                print(f"Column '{col}' does not exist in the DataFrame.")
                continue
            try:
                if data_type == 'int':
                    # Convert the column to numeric with 'coerce' option to handle non-numeric values
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
                elif data_type == 'float':
                    # Convert the column to numeric with 'coerce' option to handle non-numeric values
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)
                elif data_type == 'datetime':
                    # Convert the column to datetime, coerce errors to NaT
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                elif data_type == 'category':
                    # Convert the column to category
                    df[col] = df[col].astype('category')
                else:
                    # Convert the column to string
                    df[col] = df[col].astype(str)
            except Exception as e:
                print(f"Error converting column '{col}' to {data_type}: {e}")

    return df

def convert_dbdates_to_datetime(df):
    """
    Convert columns with date-like values in database format to datetime format.

    Parameters:
    - df: pd.DataFrame
        The DataFrame containing the potential date columns.

    Returns:
    - pd.DataFrame
        The DataFrame with date-like columns converted to datetime format.
    """
    date_columns = df.select_dtypes(include='dbdate').columns.tolist()
    for date_column in date_columns:
        # Check if the date_column exists in the DataFrame
        if date_column not in df.columns:
            print(f"Column '{date_column}' not found in the DataFrame.")
        else:
            # Convert the specified date column to datetime
            df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
    return df

def add_suffix_to_duplicates_and_rename(df):
    """
    Add a suffix to duplicate column names in a DataFrame and return the renamed DataFrame.

    Args:
        df (pandas.DataFrame): The DataFrame to be processed.

    Returns:
        pandas.DataFrame: The DataFrame with unique column names.
    """
    seen = {}
    new_columns = []
    df.columns = clean_column_names(df)
    for col in df.columns:
        col_lower_stripped = col.strip().lower()
        if col_lower_stripped in seen:
            seen[col_lower_stripped] += 1
            new_col = f"{col}_{seen[col_lower_stripped]}"
        else:
            seen[col_lower_stripped] = 1
            new_col = col

        new_columns.append(new_col)

    df.columns = new_columns
    return df

def clean_column_names(columns):
    """
    Clean column names by replacing non-alphanumeric characters with underscores and ensuring they don't start with digits.

    Args:
    columns (list): A list of column names to be cleaned.

    Returns:
    list: A list of cleaned column names.
    """
    cleaned_columns = []

    for column in columns:
        # Replace non-alphanumeric characters with underscores
        cleaned_column = re.sub(r"[^A-Za-z0-9]+", "_", column)

        # Ensure the column name doesn't start with a digit
        if cleaned_column[0].isdigit():
            cleaned_column = "_" + cleaned_column

        cleaned_columns.append(cleaned_column)

    return cleaned_columns

def calculate_averages(df, avg_columns, group_column=None):
    df=df.copy()
    if group_column:
      # Loop through columns and perform grouping
      df_col_avg = [group_data(df=df[df[agg_col] > 0], group_column=group_column, column_name=f'Avg_{agg_col}', aggregation_method='mean', aggregate_column=agg_col)
                for agg_col in avg_columns]
      df_average = reduce(lambda left, right: pd.merge(left, right, on=group_column, how='left'), df_col_avg)
      df_average.set_index(keys=group_column, inplace=True)
    else:
      # Create a dictionary to store column averages
      col_avg_data = {}
      col_avg_data['Average_type'] = 'Average'
      for agg_col in avg_columns:
          avg = df[df[agg_col] > 0][agg_col].mean().round().astype(int)
          col_avg_data[f'Avg_{agg_col}'] = avg

      # Create a new DataFrame from the dictionary
      df_average = pd.DataFrame([col_avg_data])
    return df_average

# @title Marketing Functions

def format_number(num):
    """
    Format a large number using abbreviations:
    - If the number is >= 1 billion, return the number divided by 1 billion followed by 'B'.
    - If the number is >= 1 million, return the number divided by 1 million followed by 'M'.
    - If the number is >= 1 thousand, return the number divided by 1 thousand followed by 'K'.
    - Otherwise, return the number as a string.

    Parameters:
    - num (int): The number to be formatted.

    Returns:
    - str: The formatted number.
    """
    if num >= 1_000_000_000:
        return f'{num // 1_000_000_000}B'
    elif num >= 1_000_000:
        return f'{num // 1_000_000}M'
    elif num >= 1_000:
        return f'{num // 1_000}K'
    else:
        return str(num)

# Function to calculate bins for specified columns
def calculate_bins(df, column_name, bin_edges=None, format=False):
    """
    Add a column with bucket labels to a DataFrame based on provided or default bin edges.

    Args:
    df (pandas.DataFrame): The DataFrame containing the data.
    column_name (list): The name of the columns with numeric data.
    bin_edges (list or None): A list of bin edges that define the bucket boundaries, or None for default bins.

    Returns:
    df (pandas.DataFrame): The modified DataFrame with the bucket label column.
    """
    if not isinstance(df, pd.DataFrame):
        raise ValueError("Input 'df' must be a pandas DataFrame.")

    for col in column_name:
      if col not in df.columns:
          raise ValueError(f"'{col}' not found in DataFrame columns.")

    if bin_edges is None:
        bin_edges = [-1, 0, 3, 10, 25, 50, 75, 100, float('inf')]
    else:
        bin_edges = sorted(bin_edges) + [float('inf')]

    if format:
        bucket_labels = [f"{format_number(int(bin_edges[i])+1)} to {format_number(int(bin_edges[i + 1]))}" if bin_edges[i] != float('inf') and bin_edges[i+1] != float('inf') else f"{format_number(int(bin_edges[i]))}+" for i in range(len(bin_edges) - 1)]
    else:
        bucket_labels = [f"{int(bin_edges[i])+1} to {int(bin_edges[i + 1])}" if bin_edges[i] != float('inf') and bin_edges[i+1] != float('inf') else f"{int(bin_edges[i])}+" for i in range(len(bin_edges) - 1)]


    for col in column_name:
      # Replace or drop non-finite values in the column
      df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, replacing non-finite values with NaN
      df[col] = df[col].fillna(0) # Replace NaN with 0, you can choose another value if needed
      # df[col].fillna(0, inplace=True)  
      # Add a new column with bucket labels to the DataFrame
      df[f'{col}_bins'] = pd.cut(abs(df[col]), bins=bin_edges, labels=bucket_labels, include_lowest=True)

    return df

def lowercase_strings(s):
    """
    Convert a string to lowercase.

    Args:
    s (str or any): The input string to be converted to lowercase.

    Returns:
    str or any: The input string converted to lowercase if it was a string, or the original value if it was not a string.
    """
    if isinstance(s, str):
        return s.lower()
    return s

# Function to sort a DataFrame
def sort_data(df, sort_column, ascending=True):
    """
    Sort a DataFrame based on a specified column.

    Args:
    df (DataFrame): The input DataFrame.
    sort_column (str): The column to use for sorting.
    ascending (bool, optional): Whether to sort in ascending order (default) or descending.

    Returns:
    DataFrame: The sorted DataFrame.
    """
    sorted_df = df.sort_values(by=sort_column, ascending=ascending)
    return sorted_df

# Function to drop duplicates
def drop_duplicates(df, subset=None):
    """
    Drop duplicate rows in a DataFrame.

    Args:
    df (DataFrame): The input DataFrame.
    subset (list, optional): Columns to consider when identifying duplicates.

    Returns:
    DataFrame: The DataFrame with duplicates removed.
    """
    deduplicated_df = df.drop_duplicates(subset=subset)
    return deduplicated_df

# Function to filter rows based on a condition
def filter_data(df, condition):
    """
    Filter rows in a DataFrame based on a specified condition.

    Args:
    df (DataFrame): The input DataFrame.
    condition (Series): A boolean Series used for filtering.

    Returns:
    DataFrame: The filtered DataFrame.
    """
    filtered_df = df[condition]
    return filtered_df

# Function to rename columns
def rename_columns(df, column_mapping):
    """
    Rename columns in a DataFrame based on a specified mapping.

    Args:
    df (DataFrame): The input DataFrame.
    column_mapping (dict): A dictionary that maps old column names to new column names.

    Returns:
    DataFrame: The DataFrame with renamed columns.
    """
    renamed_df = df.rename(columns=column_mapping)
    return renamed_df

# Function to add a new calculated column
def add_calculated_column(df, new_column_name, calculation_function):
    """
    Add a new calculated column to a DataFrame.

    Args:
    df (DataFrame): The input DataFrame.
    new_column_name (str): The name of the new column.
    calculation_function (function): A function to calculate values for the new column.

    Returns:
    DataFrame: The DataFrame with the new calculated column added.
    """
    df[new_column_name] = calculation_function(df)
    return df

def create_dicts(cols, col_func):
    """
    Create two dictionaries based on the input lists: one for column operations,
    and another for renamed columns reflecting those operations.

    Parameters:
    - cols: List of column names.
    - col_func: List of operations (as strings) to apply to the columns.

    Returns:
    - dict_ops: Dictionary mapping column names to their operations.
    - dict_rename: Dictionary mapping original column names to their new names,
                   reflecting the operation applied.
    """
    # Ensure the lists are of equal length
    if len(cols) != len(col_func):
        raise ValueError("The lists 'cols' and 'col_func' must be of the same length.")

    # Creating the first dictionary: column names to operations
    dict_ops = dict(zip(cols, col_func))

    # Creating the second dictionary: column names to renamed columns
    # The renaming reflects the operation applied
    name_mapping = {
        'mean': 'Avg',
        'sum': 'Sum',
        'count': '#'  # Assuming 'count' could also be a potential operation
    }
    dict_name = {}
    for col, func in zip(cols, col_func):
        new_name = f"{name_mapping.get(func, func)}_{col}"  # Default to using the function name if not found in mapping
        dict_name[col] = new_name

    return dict_ops, dict_name

def add_totals(df, group_columns=None, row_total=False, col_total=False):
    """
    Adds row totals and/or column totals to a DataFrame.

    Parameters:
    - df (pd.DataFrame): The DataFrame to modify.
    - group_columns (list or str, optional): Column name(s) to group by. Can be a single column name or a list of names.
    - row_total (bool): If True, add a row at the end with column totals.
    - col_total (bool): If True, add a column on the right with row totals.

    Returns:
    - pd.DataFrame: The modified DataFrame with requested totals added.
    """
    if group_columns and not isinstance(group_columns, list):
        group_columns = [group_columns]

    if row_total:
        if group_columns:
            cols_to_sum = df.drop(columns=group_columns)
        else:
            cols_to_sum = df
        df['RowTotal'] = cols_to_sum.sum(numeric_only=True, axis=1)

    if col_total:
        # Calculate total for numeric columns only using direct NumPy reference
        col_totals = df.select_dtypes(include=[np.number]).sum().rename('Total')

        # If there are group columns, set their values in the totals row to "Total"
        if group_columns:
            for col in group_columns:
                col_totals[col] = 'Total'

        # Ensure the total row has the same column order
        col_totals = col_totals.reindex(df.columns, fill_value='Total')

        # Append the total row to the dataframe
        df = pd.concat([df, pd.DataFrame([col_totals])])

    return df

# Function to group a DataFrame
def group_data(df, group_column, column_name, aggregation_method='count', aggregate_column=None, index=False, row_total=False, col_total=False):
    """
    Group a DataFrame and perform aggregation based on the input parameters, with options for row totals, column totals, and subtotals.

    Args:
    - df (DataFrame): The input DataFrame.
    - group_column (list): The columns to group by.
    - column_name (str or dict): New column name(s) for the aggregation result.
    - aggregation_method (str): The aggregation method, e.g., 'count', 'sum', 'unique', 'mean', 'median', 'maxid', 'agg'.
    - aggregate_column (str, optional): The column to aggregate when method is not 'count'.
    - index (bool): Whether to set the group_column as index.
    - row_total (bool): Whether to add a row total.
    - col_total (bool): Whether to add a column total.
    - add_subtotals (bool): Whether to add subtotals.

    Returns:
    - DataFrame: The grouped and aggregated DataFrame with optional totals and subtotals.
    """
    # Initial setup and replacements
    df_copy = df.copy()
    if isinstance(aggregate_column, str) or isinstance(aggregate_column, list):
        df_copy[aggregate_column] = df_copy[aggregate_column].replace({'': np.nan, 0: np.nan})
    if isinstance(aggregate_column, dict):
        for col, repl_value in aggregate_column.items():
            df_copy[col] = df_copy[col].replace({'': np.nan, 0: np.nan})

    # Group and aggregate
    if aggregation_method == 'count':
        grouped = df_copy.groupby(group_column, observed=True).size().reset_index(name=column_name)
    elif aggregation_method in ['sum', 'unique', 'mean', 'median']:
        agg_func = aggregation_method if aggregation_method != 'unique' else 'nunique'
        grouped = df_copy.groupby(group_column, observed=True).agg({aggregate_column: agg_func}).reset_index().rename(columns={aggregate_column: column_name})
    elif aggregation_method == 'maxid':
        max_id = df_copy.groupby(group_column, observed=True)[aggregate_column].idxmax()
        grouped = df_copy.loc[max_id].copy()
    elif aggregation_method == 'agg':
        grouped = df_copy.groupby(group_column, observed=True).agg(aggregate_column).reset_index()
        if isinstance(column_name, dict):
            grouped.rename(columns=column_name, inplace=True)

    # Sorting, rounding, and NaN handling
    grouped = grouped.sort_values(group_column).reset_index(drop=True)
    # grouped.fillna(0, inplace=True)
    # if aggregation_method in ['sum', 'mean', 'median', 'agg']:
    grouped = grouped.round()

    # Row and Column Totals
    if row_total or col_total:
      grouped = add_totals(df=grouped, group_columns=group_column, row_total=row_total, col_total=col_total)

    if index:
        grouped.set_index(keys=group_column, inplace=True)

    return grouped

def group_data_old(df, group_column, column_name, aggregation_method='count', aggregate_column=None, index=False, row_total=None, col_total=None):
    """
    Group a DataFrame and perform aggregation based on the input parameters.

    Args:
    df (DataFrame): The input DataFrame.
    group_column (str or list): The column(s) to group by.
    aggregation_method (str): The aggregation method, 'count' or 'sum'.
    aggregate_column (str, optional): The column to aggregate when method is 'sum'.

    Returns:
    DataFrame: The grouped and aggregated DataFrame.
    """
    df = df.copy()
    # Replace Blanks or 0 by NaN so they are not counted
    if isinstance(aggregate_column, str):
      df[aggregate_column] = df[aggregate_column].replace({'': np.nan, 0: np.nan})
    if isinstance(aggregate_column, dict):
      df[list(aggregate_column.keys())] = df[list(aggregate_column.keys())].replace({'': np.nan, 0: np.nan})

    if aggregation_method == 'count':
        result = df.groupby(group_column)[aggregate_column].size().reset_index(name=column_name).sort_values(by=column_name, ascending=False)
    elif aggregation_method == 'sum':
        result = df.groupby(group_column)[aggregate_column].sum(numeric_only=True).reset_index(name=column_name).fillna(0).round()
    elif aggregation_method == 'unique':
        result = df.groupby(group_column)[aggregate_column].nunique().reset_index(name=column_name).fillna(0).round()
    elif aggregation_method == 'mean':
        result = df.groupby(group_column)[aggregate_column].mean().reset_index(name=column_name)
        result[column_name] = result[column_name].fillna(0).round().astype(int)
    elif aggregation_method == 'median':
        result = df.groupby(group_column)[aggregate_column].median().reset_index(name=column_name).fillna(0).round(2)
    elif aggregation_method == 'maxid':
        result = df.loc[df.groupby(group_column)[aggregate_column].idxmax()]
    elif aggregation_method == 'agg':
        result = df.groupby(group_column).agg(aggregate_column).reset_index().round()
        result.rename(columns=column_name, inplace=True)
    else:
        raise ValueError("Unsupported aggregation method. Use 'count' or 'sum'.")

    sorted_df = sort_data(result, group_column)

    if row_total:
      sorted_df['RowTotal'] = sorted_df.drop(group_column, axis=1).sum(numeric_only=True, axis=1)

    if col_total:
      sorted_df = pd.concat([sorted_df,pd.DataFrame(sorted_df.sum(numeric_only=True, axis=0),columns=['Grand Total']).T], ignore_index=True)

    # Handle Na
    sorted_df = fillna_custom(sorted_df)

    if index:
      sorted_df.set_index(keys=group_column, inplace=True)

    return sorted_df

# Function to pivot a DataFrame
def pivot_data(df, index_column, pivot_column, column_name, aggfunc='sum'):
    """
    Pivot a DataFrame based on specified index and pivot columns.

    Args:
    df (DataFrame): The input DataFrame.
    index_column (str): The column to use as the index.
    pivot_column (str): The column to use for pivoting.

    Returns:
    DataFrame: The pivoted DataFrame.
    """
    pivoted_df = df.pivot_table(index=index_column, columns=pivot_column, values=column_name, aggfunc=aggfunc, fill_value=0, observed=True).round().astype(int)
    return pivoted_df

# Function to pivot and group data in a single step
def pivot_group_data(df, group_column, pivot_column, column_name, aggregation_method='count', aggregate_column=None, drop_column=None, row_total=None, col_total=None):
    """
    Pivot and group data in a single step, with optional aggregation and column dropping.

    Args:
    df (DataFrame): The input DataFrame.
    group_column (str or list): The column(s) to group by.
    pivot_column (str): The column to pivot.
    aggregation_method (str): The aggregation method, 'count' or 'sum'.
    aggregate_column (str, optional): The column to aggregate when method is 'sum'.
    drop_column (list, optional): Columns to drop before performing the operation.

    Returns:
    DataFrame: The transformed and pivoted DataFrame.
    """
    group_columns = group_column + pivot_column
    transformed_df = df.copy()
    if drop_column:
        drop_columns = drop_column + group_column + pivot_column
        transformed_df = drop_duplicates(transformed_df, subset=drop_columns)

    transformed_df = group_data(transformed_df, group_column=group_columns, aggregation_method=aggregation_method, column_name=column_name, aggregate_column=aggregate_column)
    if isinstance(column_name, dict):
      transformed_df = pivot_data(transformed_df, index_column=group_column, pivot_column=pivot_column, column_name=None, aggfunc='sum')
    else:
      transformed_df = pivot_data(transformed_df, index_column=group_column, pivot_column=pivot_column, column_name=column_name, aggfunc='sum')

    if row_total or col_total:
      transformed_df = add_totals(df=transformed_df, group_columns=None, row_total=row_total, col_total=col_total)

    return transformed_df

# @title Marketing Functions

def calculate_split_amounts(df, weight_columns, amount_column, equal_weight=False, equal_amount=False):
    """
    Calculate split amounts based on the provided weight columns for each record in a DataFrame.

    Parameters:
        df (pandas.DataFrame): The input DataFrame containing weight and amount columns.
        weight_columns (list of str): A list of column names containing the weights.
        amount_column (str): The name of the column containing the total amount to be split.

    Returns:
        pandas.DataFrame: A DataFrame with split amount columns added for each weight, as well as a 'TotalSplitAmount' column.

    Example:
        data = {
            'Item': ['Item 1', 'Item 2', 'Item 3'],
            'Amount': [100, 50, 200],
            'Weight1': [1, 2, 3],
            'Weight2': [5, 7, 9],
            'Weight3': [0.3, 0.5, 0.2]
        }

        df = pd.DataFrame(data)
        weight_columns = ['Weight1', 'Weight2', 'Weight3']
        amount_column = 'Amount'
        df_with_split = calculate_split_amounts(df, weight_columns, amount_column)
    """

    df = df.copy()
    # Create a list to store the split amount columns
    split_amount_columns = [f'{weight_col}_ACV' for weight_col in weight_columns]

    # Update columns
    if equal_weight:
      df[weight_columns] = df[weight_columns].applymap(lambda x: 1 if x > 0 else 0)

    total_weights = df[weight_columns].sum(axis=1)

    for weight_col in weight_columns:
        df[f'{weight_col}_ACV'] = round(df[amount_column] * df[weight_col] if equal_amount else (df[amount_column] * df[weight_col] / total_weights), 2)

    df.fillna(0, inplace=True)

    # Ensure that the sum of split amounts is equal to the original 'Amount'
    df['TotalSplitAmount'] = round(df[split_amount_columns].sum(axis=1), 2)
    df['TotalSplitAmount(M)'] = df['TotalSplitAmount'].apply(lambda x: '${:.2f}M'.format(x/1_000_000))

    # # Add column totals
    # df.loc['Column Total'] = round(df.sum(numeric_only=True), 2)
    # # df['Column Total'] = df['Column Total'].apply(lambda x: '${:.2f} million'.format(x/1_000_000))

    bin_columns = split_amount_columns + ['TotalSplitAmount']

    #Calculate Bins
    revenue_bins = [-1, 0, 1_000_000, 10_000_000, 100_000_000, 300_000_000, 500_000_000, 1_000_000_000]

    df = calculate_bins(df, column_name=bin_columns, bin_edges=revenue_bins, format=True)
    # for col in bin_columns:
    #   df = calculate_bins(df, col, bin_edges=[-1, 0, 250000, 500000, 750000])

    return df

def calculate_deal_velocity(df, stages, date_format=None):
    """
    Calculates the time in days between each stage transition and the overall deal velocity
    from the first stage to the last stage for opportunities within a DataFrame. It also ensures
    all date columns are properly converted to datetime objects.

    Parameters:
    - df: A pandas DataFrame containing the opportunities data.
    - stages: A list of strings representing the column names for each stage in the sales process.
              This list should include the 'Created Date' at the start and 'Close Date' at the end.
    - date_format: An optional string specifying the format of the dates in the DataFrame columns.
                   This is necessary if the automatic conversion doesn't work as expected.

    Returns:
    - A modified DataFrame that includes additional columns showing the velocity (in days) between
      each pair of stages and the overall velocity from the first to the last stage.
    """

    # Convert all specified stage columns to datetime objects
    if date_format:
      for stage in stages:
        try:
          df[stage] = pd.to_datetime(df[stage])
        except Exception as e:
            print(f"Error converting {stage} to datetime: {e}")

    # Calculate the time between each pair of stages
    for i in range(len(stages) - 1):
        df[f"Velocity_{stages[i]}_to_{stages[i + 1]}"] = (df[stages[i + 1]] - df[stages[i]]).dt.days

    # Calculate overall velocity for the specified outcome
    df[f"Velocity_{stages[0]}_to_{stages[-1]}"] = df.apply(lambda x: (x[stages[-1]] - x[stages[0]]).days, axis=1)
    df[f"Velocity_{stages[1]}_to_{stages[-1]}"] = df.apply(lambda x: (x[stages[-1]] - x[stages[1]]).days, axis=1)

    return df

def calculate_split_amounts_old(df, weight_columns, amount_column):
    """
    Calculate split amounts based on the provided weight columns for each record in a DataFrame.

    Parameters:
        df (pandas.DataFrame): The input DataFrame containing weight and amount columns.
        weight_columns (list of str): A list of column names containing the weights.
        amount_column (str): The name of the column containing the total amount to be split.

    Returns:
        pandas.DataFrame: A DataFrame with split amount columns added for each weight, as well as a 'TotalSplitAmount' column.

    Example:
        data = {
            'Item': ['Item 1', 'Item 2', 'Item 3'],
            'Amount': [100, 50, 200],
            'Weight1': [1, 2, 3],
            'Weight2': [5, 7, 9],
            'Weight3': [0.3, 0.5, 0.2]
        }

        df = pd.DataFrame(data)
        weight_columns = ['Weight1', 'Weight2', 'Weight3']
        amount_column = 'Amount'
        df_with_split = calculate_split_amounts(df, weight_columns, amount_column)
    """
    # Create a list to store the split amount columns
    split_amount_columns = [f'{weight_col}_ACV' for weight_col in weight_columns]

    total_weights = df[weight_columns].sum(axis=1)  # Calculate total weights for each row
    for weight_col in weight_columns:
        df[f'{weight_col}_ACV'] = round((df[amount_column] * df[weight_col]) / total_weights, 2)

    # Ensure that the sum of split amounts is equal to the original 'Amount'
    df['TotalSplitAmount'] = df[split_amount_columns].sum(axis=1)

    return df

def campaign_touch_by_stage(df, unique_activity=None, filter_df=None):

  # Define the condition for 'Stage1' column
  stage1_touch = (df['CM_FirstRespondedDate'] >= df['OPP_AdustedCreatedDate']) & \
                  (df['CM_FirstRespondedDate'] <= df['OPP_Stage1Date'])

  # Create a new column 'Stage1' based on the condition
  df.loc[:, 'Stage1Touch'] = np.where(stage1_touch, 1, 0)

  # Define the condition for 'Stage2' column
  stage2_touch = (df['CM_FirstRespondedDate'] > df['OPP_Stage1Date']) & \
                  (df['CM_FirstRespondedDate'] <= df['OPP_Stage2Date'])

  # Create a new column 'Stage2' based on the condition
  df.loc[:, 'Stage2Touch'] = np.where(stage2_touch, 1, 0)

  # Define the condition for 'Stage3' column
  stage3_touch = (df['CM_FirstRespondedDate'] > df['OPP_Stage2Date']) & \
                  (df['CM_FirstRespondedDate'] <= df['OPP_Stage3Date'])

  # Create a new column 'Stage3' based on the condition
  df.loc[:, 'Stage3Touch'] = np.where(stage3_touch, 1, 0)

  # Define the condition for 'Stage4' column
  stage4_touch = (df['CM_FirstRespondedDate'] > df['OPP_Stage3Date']) & \
                  (df['CM_FirstRespondedDate'] <= df['OPP_Stage4Date'])

  # Create a new column 'Stage1' based on the condition
  df.loc[:, 'Stage4Touch'] = np.where(stage4_touch, 1, 0)

  # Define the condition for 'Stage5' column
  stage5_touch = (df['CM_FirstRespondedDate'] > df['OPP_Stage4Date']) & \
                  (df['CM_FirstRespondedDate'] <= df['OPP_CloseDate'])

  # Create a new column 'Stage1' based on the condition
  df.loc[:, 'Stage5Touch'] = np.where(stage5_touch, 1, 0)

  df['StageTouch'] = df[['Stage1Touch', 'Stage2Touch', 'Stage3Touch', 'Stage4Touch', 'Stage5Touch']].sum(axis=1)

  if unique_activity:
    df.drop_duplicates(subset=['CM_MemberId','StageTouch'], inplace=True)

  # filter Campaign Members within the Opportunity duration
  if filter_df:
    condition = (df['CM_FirstRespondedDate'] >= df['OPP_AdustedCreatedDate']) & \
              (df['CM_FirstRespondedDate'] <= df['OPP_CloseDate'])
    df = df[condition]

  return df

def by_stage(df, stages, group_column, output_column, pivot_column=None):

    stage_dfs = { }
    stage_avg_dfs = []

    # Define the desired order of columns
    ordered = ['C-Level', 'SVP', 'VP', 'Director', 'Others']

    for stage in stages:
        # Filter DataFrame for the current stage
        df_stage = df[df[stage] > 0]

        if pivot_column:
          # Pivot and aggregate data for the current stage
          df_stage_touches = pivot_group_data(df=df_stage,
                                              group_column=group_column,
                                              pivot_column=pivot_column,
                                              column_name=stage,
                                              aggregation_method='count',
                                              aggregate_column='CM_MemberId',
                                              row_total=None,
                                              col_total=None)
        else:
          # Pivot and aggregate data for the current stage
          df_stage_touches = group_data(df=df_stage,
                                        group_column=group_column,
                                        column_name=stage,
                                        aggregation_method='count',
                                        aggregate_column='CM_MemberId',
                                        row_total=None,
                                        col_total=None)

        # Get the column names to average
        cols = list(df_stage_touches.select_dtypes(include='number'))
        # Reorder columns based on the desired order
        if ordered in cols:
          cols = [col for col in ordered if col in cols]

        # Create a list of operations to perform
        ops = ['mean' for _ in cols]

        aggregate_column, column_name = create_dicts(cols, ops)
        # Calculate averages for the current stage
        df_avg_stage_touches = group_data(df=df_stage_touches,
                                          group_column=output_column,
                                          column_name=column_name,
                                          aggregation_method='agg',
                                          aggregate_column=aggregate_column,
                                          index=False,
                                          row_total=None,
                                          col_total=None)

        # Store the DataFrame for the current stage in the dictionary
        stage_dfs[stage] = df_stage_touches

        df_avg_stage_touches.set_index(output_column, inplace=True)

        # Calculate the mean for numeric columns only
        numeric_columns = df_avg_stage_touches.select_dtypes(include=['number']).columns

        # Calculate the average for each column
        column_averages = df_avg_stage_touches[numeric_columns].mean().round()

        # Create a new DataFrame with the averages
        averages_df = pd.DataFrame([column_averages], index=['Total Average'])

        # Concatenate the averages DataFrame to the original DataFrame
        df_avg_stage_touches = pd.concat([df_avg_stage_touches, averages_df])
        stage_avg_dfs.append(df_avg_stage_touches)

    df_stages_avg = pd.concat(stage_avg_dfs, axis=1)

    return stage_dfs, df_stages_avg

def channels_by_stage(df, stages, group_column, pivot_column, output_column):

    stage_dfs = {}  # Dictionary to store DataFrames for each stage
    stage_avg_dfs = {}

    for stage in stages:
        # Filter DataFrame for the current stage
        df_stage = df[df[stage] > 0]

        # Pivot and aggregate data for the current stage
        df_stage_touches = pivot_group_data(df=df_stage,
                                            group_column=group_column,
                                            pivot_column=pivot_column,
                                            column_name=stage,
                                            aggregation_method='count',
                                            aggregate_column='CM_MemberId',
                                            row_total=True,
                                            col_total=None)

        # Get the column names to average
        cols = list(df_stage_touches.columns)
        cols.remove('RowTotal')

        # Desired order
        ordered = ['C-Level', 'SVP', 'VP', 'Director', 'Others']

        if ordered in cols:
          # Reorder the original list by the index of each item in the desired order list
          cols = sorted(cols, key=lambda x: ordered.index(x))

        # Create a list of operation to perform
        ops = ['mean' for _ in cols]

        aggregate_column, column_name  = create_dicts(cols, ops)

        # Calculate averages for the current stage
        df_avg_stage_touches = group_data(df=df_stage_touches,
                                          group_column=output_column,
                                          column_name=column_name,
                                          aggregation_method='agg',
                                          aggregate_column=aggregate_column,
                                          row_total=None,
                                          col_total=None)

        df_avg_stage_touches.set_index(output_column, inplace=True)

        # Calculate the mean for numeric columns only
        numeric_columns = df_avg_stage_touches.select_dtypes(include=['number']).columns

        # Calculate the average for each column
        column_averages = df_avg_stage_touches[numeric_columns].mean().round()

        # Create a new DataFrame with the averages
        averages_df = pd.DataFrame([column_averages], index=['Total Average'])

        # Concatenate the averages DataFrame to the original DataFrame
        df_avg_stage_touches = pd.concat([df_avg_stage_touches, averages_df])

        # Store the DataFrame for the current stage in the dictionary
        stage_dfs[stage] = datatable(df_stage_touches)
        stage_avg_dfs[stage] = datatable(df_avg_stage_touches)

    return stage_dfs, stage_avg_dfs

def get_top_campaigns_by_stage(df, stages, top=5):
    top_campaigns = []
    for stage in stages:
        query = f'{stage}Touch > 0'
        col = f'{stage}_Channels'

        # Assuming group_data() function correctly aggregates data
        df_campaigns_by_stage = group_data(df=df.query(query, engine='python'), group_column=['CM_CampaignChannels'], column_name='Count', aggregation_method='count', aggregate_column=None, index=False, row_total=False, col_total=False)

        df_campaigns_by_stage.rename(columns={'CM_CampaignChannels': col}, inplace=True)
        df_campaigns_by_stage = df_campaigns_by_stage.sort_values('Count', ascending=False)
        top_campaigns.append(df_campaigns_by_stage[col].head(top).reset_index(drop=True))

    df_campaigns_by_stages = pd.concat(top_campaigns, axis=1)
    return df_campaigns_by_stages

def get_top_campaigns_by_stage_col(df, column, opp_stages, top):
    df_campaigns_all = pd.DataFrame()  # Initialize an empty DataFrame to hold results

    for col in df[column].unique():
        # Filter the dataframe by the unique 'col' value
        df_filtered = df[df[column] == col]
        
        # Call the utility function to get top campaigns for the specific 'col'
        df_campaigns = get_top_campaigns_by_stage(df_filtered, stages=opp_stages, top=top)
        
        # Add a new column with the 'col' value
        df_campaigns[column] = col
        
        # Concatenate the result with the main DataFrame
        df_campaigns_all = pd.concat([df_campaigns_all, df_campaigns], ignore_index=True)

    # Reorder the columns to move the specified column to the start
    columns_order = [column] + [col for col in df_campaigns_all.columns if col != column]
    df_campaigns_all = df_campaigns_all[columns_order]
    df_campaigns_all.set_index(column, inplace=True)
    return df_campaigns_all

def get_top_campaigns_by_stage_group(df, out_column, stages, top=5):
    group_column='CM_CampaignChannels'
    group_columns = [group_column] + [out_column]
    out_columns = [out_column]
    top_campaigns = []
    for stage in stages:
        query = f'{stage}Touch > 0'
        col = f'{stage}_Channels'

        # Assuming group_data() function correctly aggregates data
        df_campaigns_by_stage = group_data(df=df.query(query), group_column=group_columns, column_name='Count', aggregation_method='count', aggregate_column=None, index=False, row_total=False, col_total=False)
        df_campaigns_by_stage = df_campaigns_by_stage.sort_values([out_column, 'Count'], ascending=False)
        df_campaigns_by_stage = df_campaigns_by_stage.groupby(out_column).head(top).reset_index(drop=True)
        df_campaigns_by_stage.rename(columns={group_column: col}, inplace=True)
        df_campaigns_by_stage.set_index(out_column, inplace=True)
        top_campaigns.append(df_campaigns_by_stage)
        out_columns.append(col)

    df_campaigns_by_stages = pd.concat(top_campaigns, axis=1)

    # Fill NaN values with an empty string
    df_campaigns_by_stages.fillna('', inplace=True)
    return df_campaigns_by_stages

def clean_website_domain(url):
    """
    Clean and extract the website and domain from a URL using vectorized operations.

    Args:
    url (str): A single URL to be cleaned and processed.

    Returns:
    tuple: A tuple containing two strings - the cleaned website and the extracted domain.
    """
    if url:
        # Define a regex pattern to match common URL prefixes and ports
        pattern = r'^((?:https?://)?(?:www\.)?)|(?:\:\d{2,5})'

        # Use regex-based replacement to clean the URL
        cleaned_url = re.sub(pattern, '', url.strip())

        # Split the cleaned URL into website and domain parts
        parts = cleaned_url.split('/', 1)

        # Extract website and domain
        website = parts[0]

        if not website.startswith('www.'):
            domain = website
            website = 'www.' + website
        else:
          domain = website.split('www.')

        return website, domain

    else:
        return None, None

def fix_job_level(df, column_name, replace):
    job_levels = ['C-Level', 'Director', 'Manager', 'SVP', 'VP']

    if replace:
        jl_mapping = {
            'C Level': 'C-Level',
            'C-Suite': 'C-Level',
            'VP Level': 'VP',
            '': 'Others'
            }

        # Update Job Level
        df[column_name] = df[column_name].replace(jl_mapping)

    # Update values to 'Others' if not in the list
    df.loc[~df[column_name].isin(job_levels), column_name] = 'Others'

    return df

def fix_job_function(df, column_name, replace):
    df[column_name] = df[column_name].fillna('Others')

    job_functions = ['Finance', 'Marketing', 'Sales', 'Supply Chain', 'Information Technology', 'Human Resources', 'G&A']

    if replace:
        # Define mapping for different categories
        jf_mapping = {
            'Finance': ['finance', 'financial', 'accounting', 'treasury', 'tax'],
            'Marketing': ['marketing', 'Demand Generation', 'Brand Management', 'Corporate Strategy', 'Strategic Communications', 'Business Development', 'Lead Generation', 'Customer', 'Advertising' ],
            'Sales': ['sales', 'executive', 'account'],
            'Supply Chain': ['logistics', 'operations', 'procurement'],
            'Information Technology': ['IT', 'devops', 'software', 'ui', 'ux', 'Information Technology', 'data', 'service', 'support', 'engineer', 'tech', 'product', 'project', 'machine', 'artificial', 'intelligence', 'engine', 'application', 'ecommerce', 'research', 'Networking'],
            'Human Resources': ['workforce', 'talent', 'human', 'resource', 'employee', 'pr'],
            'G&A': ['legal', 'risk', 'Governmental Affairs & Regulatory Law', 'Leasing', 'Organizational Development'],
        }

        # Iterate through categories and update JobFunction
        for category, keywords in jf_mapping.items():
            pattern = '|'.join(keywords)
            df.loc[df[column_name].str.contains(pattern, case=False), column_name] = category

    # Update values to 'Others' if not in the list
    df.loc[~df[column_name].isin(job_functions), column_name] = 'Others'

    return df

# @title RecordMatcher
class RecordMatcher:
    """
    A class for matching records in two dataframes using the RecordLinkage library.

    Args:
    df1 (pandas.DataFrame): The first dataframe to match.
    df2 (pandas.DataFrame): The second dataframe to match.
    match_type (str): Defines the matching criteria.

    Example:
    matcher = RecordMatcher(df1, df2, 'name')
    df_matches = matcher.get_potential_matches()
    """

    def __init__(self, df1, df2, match_type):
        self.df1 = df1
        self.df2 = df2
        self._define_matching_rules(match_type)
        self._find_missing_cols()

    def _create_indexer(self):
        """
        Create a recordlinkage indexer object based on the matching rules.

        Returns:
        recordlinkage.Index: A recordlinkage indexer object.
        """
        indexer = rl.Index()
        index_rules = self.matching_rules.get('index', {})

        # Process 'block' rules
        block_rules = index_rules.get("block", {})
        for right_on, left_on in block_rules.items():
          for col in left_on:
            indexer.block(left_on=col, right_on=right_on)

        # Process 'sortedneighbour' rules
        sortedneighbour_rules = index_rules.get("sortedneighbour", {})
        for right_on, left_on in sortedneighbour_rules.items():
          for col in left_on:
            indexer.sortedneighbourhood(left_on=col, right_on=right_on)

        print(f"Blocks: {indexer.algorithms}.")
        return indexer

    def _create_comparator(self, n_jobs=4):
        """
        Create a recordlinkage comparator object based on the matching rules.

        Args:
        n_jobs (int, optional): The number of parallel jobs to run. Default is 4.

        Returns:
        recordlinkage.Compare: A recordlinkage comparator object.
        """
        comparison = rl.Compare(n_jobs=n_jobs)
        compare_rules = self.matching_rules.get('compare', {})

        for right_on, left_on in compare_rules.items():
            label = right_on.replace("SF_", "Match_")
            for col in left_on:
                if self.df1[col].dtype == self.df2[right_on].dtype:
                    if self.df1[col].dtype == 'object':
                        method = "jarowinkler"
                        comparison.string(col, right_on, method=method, threshold=0.85, label=f"{label}_{col}")
                    else:
                        comparison.exact(col, right_on, label=f"{label}_{col}")
                else:
                    print(f"Warning: Column '{col}' in df1 and column '{right_on}' in df2 have different data types.")

        print(f"Comparison Features: {comparison.features}.")
        return comparison

    def get_potential_matches(self):
        """
        Identify potential matches between two dataframes.

        Returns:
        pandas.DataFrame: A dataframe of potential matches.
        """
        indexer = self._create_indexer()
        comparator = self._create_comparator()
        candidate_links = indexer.index(self.df1, self.df2)

        print(f"Potential Duplicates Found: {len(candidate_links)}.")

        features = comparator.compute(candidate_links, self.df1, self.df2)
        features = features[features.sum(axis=1) >= 1].reset_index()
        features["Match_zScore"] = features.filter(like="Match_").sum(axis=1)

        # Format the output dataframe
        self._format_output(features)
        return self.features

    def _format_output(self, df_matches):
        """
        Format the output dataframe.

        Args:
        df_matches (pandas.DataFrame): Dataframe containing potential matches.
        """
        self.features = pd.merge(self.df1, df_matches, on=self.df1.index.name, how='left')
        self.features = pd.merge(self.features, self.df2, on=self.df2.index.name, how='left', suffixes=('_df1', '_df2'))
        match_sf_cols = [col for col in self.features.columns if col.startswith('Match_') or col.startswith('SF_')]
        inp_cols = [self.df1.index.name] + list(self.df1.columns)
        out_cols = inp_cols + sorted(match_sf_cols)
        self.features = self.features[out_cols].fillna(0)
        self.features.reset_index(drop=True, inplace=True)

    def _define_matching_rules(self, match_type):
        """
        Define matching rules based on the match type.

        Returns:
        dict: Matching rules dictionary.
        """
        if match_type == "ZoomInfo":
            self.matching_rules = {
                "index": {"block": {"SF_AccountId": ["ZI_AccountId"]},
                          "sortedneighbour": {}},
                "compare": {
                    "SF_AccountName": ["ZI_AccountName"],
                    "SF_WebsiteClean": ["ZI_WebsiteClean"],
                    "SF_DomainClean": ["ZI_DomainClean"],
                    "SF_BillingCity": ["ZI_City"],
                    "SF_BillingState": ["ZI_State"],
                    "SF_BillingPostalCode": ["ZI_PostalCode"],
                    "SF_StandardizedCountry": ["ZI_StandardizedCountry"],
                },
            }
        elif match_type == "name":
            self.matching_rules = {
                "index": {"block": {},
                          "sortedneighbour": {"SF_AccountName": ["INP_Company", "INP_Outreach_Account_Natural_Name"],
                                              "SF_CleanName": ["INP_Outreach_Account_Natural_Name"]
                                              }},
                "compare": {
                    "SF_AccountName": ['INP_Company', 'INP_Outreach_Account_Natural_Name'],
                    "SF_CleanName": ['INP_Outreach_Account_Natural_Name'],
                },
            }
        else:
            raise ValueError(f"Unknown match_type: {match_type}")

    def _find_missing_cols(self):
        """
        Find missing labels in two sets of columns based on a mapping of column names.

        Returns:
        tuple: Two lists of missing labels in df1 and df2, respectively.
        """
        df1_cols, df2_cols = self._extract_df_columns()
        df1_missing_cols = [col for col in df1_cols if col not in self.df1.columns]
        df2_missing_cols = [col for col in df2_cols if col not in self.df2.columns]

        print(f"Missing labels in df1: {df1_missing_cols}")
        print(f"Missing labels in df2: {df2_missing_cols}")

        return df1_missing_cols, df2_missing_cols

    def _extract_df_columns(self):
        """
        Extract leaf keys and values from the matching rules.

        Returns:
        tuple: Two lists containing leaf keys and values.
        """
        df1_cols = []
        df2_cols = []

        def recurse(data_dict):
            for key, value in data_dict.items():
                # Use recursion to handle nested dictionaries
                if isinstance(value, dict):
                    recurse(value)
                else:
                    # Process lists or scalar values directly
                    if isinstance(value, list):
                        filtered_values = [v for v in value if v is not None]
                        if filtered_values:  # Only add key if there are non-None values
                            df2_cols.extend([key] * len(filtered_values))
                            df1_cols.extend(filtered_values)
                    elif value is not None:  # Handle scalars directly
                        df2_cols.append(key)
                        df1_cols.append(value)

        recurse(self.matching_rules)
        return df1_cols, df2_cols

# @title CleanBusinessName

class CleanBusinessName:
    def __init__(self, bigquery_client):
        """Initialize with a BigQuery client."""
        self.client = bigquery_client
        self._stopwords = None
        self._countries = None
        self._stopwords_pattern = None
        self._country_pattern = None

    def _fetch_stopwords(self):
        """Fetch stopwords from a BigQuery table and cache them."""
        if self._stopwords is None:
            query = """
            SELECT string_field_0 FROM `ap-marketing-data-ops-prod.MaintenanceDB.Stopwords`
            """
            results = execute_query(self.client, query)
        return [row for row in results.string_field_0]

    def _fetch_countries(self):
        """Fetch country names from a BigQuery table, escape them, and cache."""
        if self._countries is None:
            query = """
            SELECT string_field_0 FROM `ap-marketing-data-ops-prod.MaintenanceDB.Countries`
            """
            results = execute_query(self.client, query)
        return [re.escape(name) for name in results.string_field_0]

    def _compile_pattern(self, pattern_type):
        """Compile and cache regex patterns for stopwords and countries."""
        if pattern_type == 'stopwords' and self._stopwords_pattern is None:
            self._stopwords_pattern = re.compile(r'\b(?:' + '|'.join(self._fetch_stopwords()) + r')\b', flags=re.IGNORECASE)
        elif pattern_type == 'country' and self._country_pattern is None:
            self._country_pattern = re.compile(r'\b(?:' + '|'.join(self._fetch_countries()) + r')\b', flags=re.IGNORECASE)
        return getattr(self, f"_{pattern_type}_pattern")

    def clean_business_name(self, name):
        """Clean the business name using a hypothetical external cleaning library."""
        clean = CleanCorp(name)  # Assuming CleanCorp is implemented elsewhere
        return clean.clean_name.title()

    def apply_regex_patterns(self, text):
        """Apply generic regex transformations to clean text."""
        patterns = [r"\([^()]*\)|\[[^[\]]*\]|\([^()]*$|\[[^[\]]*$", r"[,-]", r"\s+"]

        for pattern in patterns:
            text = re.sub(pattern, " ", text, flags=re.MULTILINE)
        return text.strip()

    def clean_names(self, names):
        """Apply all cleaning steps to a pandas Series of names."""
        def clean_text(text):
            text = self.clean_business_name(text)
            text = self.apply_regex_patterns(text)
            text = re.sub(self._compile_pattern('stopwords'), '', text).strip()
            text = re.sub(self._compile_pattern('country'), '', text).strip()
            return text

        return names.apply(clean_text)

# @title Record Matching

def find_missing_labels(fields_dict, col1, col2):
    # Get the keys from the fields_dict
    keys = list(fields_dict.keys())
    values = list(fields_dict.values())

    # Find missing keys
    missing_keys = [x for x in keys if x not in col2]
    missing_values = [x for x in values if x not in col1]

    if missing_keys:
        print(f"Missing Keys: {keys, missing_keys}.")
    if missing_values:
        print(f"Missing Values: {values, missing_values}.")

    return missing_keys, missing_values

def Potential_Matches(df_accounts, all_accounts, fields_dict):

    # Apply the function to all string columns
    df_accounts = df_accounts.applymap(lowercase_strings)
    all_accounts = all_accounts.applymap(lowercase_strings)

    find_missing_labels(fields_dict, list(df_accounts.columns), list(all_accounts.columns))

    indexer = rl.Index()
    match_cols = []
    if 'SF_AccountName' in fields_dict:
      match_cols.append('Match_AccountName')
      indexer.sortedneighbourhood(
          left_on=fields_dict.get('SF_AccountName'),
          right_on='SF_AccountName')

    if 'SF_CleanName' in fields_dict:
      match_cols.append('Match_CleanName')
      indexer.block(
          left_on=fields_dict.get('SF_AccountName'),
          right_on='SF_CleanName')
      # indexer.block(
      #     left_on=fields_dict.get('SF_CleanName'),
      #     right_on='SF_CleanName')
      indexer.sortedneighbourhood(
          left_on=fields_dict.get('SF_CleanName'),
          right_on='SF_CleanName')

    if 'SF_CleanWebsite' in fields_dict:
      match_cols.append('Match_CleanWebsite')
      indexer.block(
          left_on=fields_dict.get('SF_CleanWebsite'),
          right_on='SF_CleanWebsite')

    # if 'SF_BillingCountry' in fields_dict:
    #   match_cols.append('Match_BillingCountry')

    compare = CompareStrings(fields_dict)
    candidate_links = indexer.index(df_accounts, all_accounts)


    print(f"Blocks: {indexer.algorithms}. ")
    print(f"Comparison Features: {compare.features}. ")
    print(f"Potential Duplicates Found: {len(candidate_links)}. ")
    print(f"Match Columns: {match_cols}")
    features = compare.compute(candidate_links, df_accounts, all_accounts)

    features.sum(axis=1).value_counts().sort_index(ascending=False)

    potential_duplicates = features[features.sum(axis=1) >= 1].reset_index()

    # Filter rows where the sum of specified columns is greater than 0
    potential_duplicates = potential_duplicates[potential_duplicates[match_cols].sum(axis=1) > 0]

    # Sum the selected columns along axis 1 and assign it to a new column
    potential_duplicates["Match_Score"] = potential_duplicates.filter(like="Match_").sum(axis=1)


    potential_duplicates.reset_index(drop=True, inplace=True)

    return potential_duplicates

def CompareStrings(fields_dict):
    n_jobs = 8
    comparison = rl.Compare(n_jobs=n_jobs)

    for key, value in fields_dict.items():
        label = f"Match_{key.replace('SF_','')}"
        comparison.string(
            value,
            key,
            method="jarowinkler" if key == 'SF_CleanName' else "damerau_levenshtein",
            threshold=0.85,
            label=label,
        )
    return comparison


def Format_output(potential_duplicates, df_accounts, all_accounts, fields_dict):
    print(f"Formatting Output.")

    df_duplicates = pd.merge(
        df_accounts, potential_duplicates, on=df_accounts.index.name, how="left"
    )

    df_duplicates = pd.merge(
        df_duplicates,
        all_accounts,
        on="SF_AccountId",
        how="left"
    )
    df_duplicates.columns = clean_column_names(df_duplicates.columns)

    # Extract column names that start with "Match_" or "SF_"
    match_sf_cols = [col for col in df_duplicates.columns if col.startswith('Match_') or col.startswith('SF_')]

    # Extract the Input columns columns
    inp_cols = [ df_accounts.index.name ] + list(df_accounts.columns) #[x for x in list(fields_dict.values())]

    # Concatenate the columns
    out_cols = inp_cols + match_sf_cols

    # Create a new DataFrame with out columns
    df_duplicates = df_duplicates[out_cols]

    df_duplicates = fillna_custom(df_duplicates)

    df_duplicates.reset_index(drop=True, inplace=True)

    return df_duplicates

# Define a function to convert a string to lowercase
def lowercase_strings(x):
    if isinstance(x, str):
        return x.lower()
    return x

def field_mappings(type):
  if type == "ZoomInfo":
    # ZoomInfo Mapping
    fields_dict = {'SF_AccountName': {'on': 'ZI_AccountName',
                                     'index': 'sortedneighbourhood'},
                  'SF_WebsiteClean': {'on': 'ZI_WebsiteClean'},
                  'SF_DomainClean': {'on': 'ZI_DomainClean'},
                  'SF_BillingCity': {'on': 'ZI_City'},
                  'SF_BillingState': {'on': 'ZI_State'},
                  'SF_BillingPostalCode': {'on': 'ZI_PostalCode'},
                  'SF_BillingCountry': {'on': 'ZI_Country',
                                        'index': 'block'}
                  }
  if type == "adhoc":
    # Adhoc Mapping
    fields_dict = {
                  'SF_AccountName': 'INP_COMPANY',
                  'SF_CleanName': 'INP_Outreach_Account_Natural_Name',
                  # 'SF_CleanWebsite': 'INP_CleanWebsite',
                  # 'SF_BillingCity': 'INP_Company_City',
                  'SF_BillingState': 'INP_STATE',
                  # 'SF_BillingPostalCode': 'INP_Company_Zip_Code',
                  'SF_BillingCountry': 'INP_COUNTRY'
                  }

  if type == "name":
    # Adhoc Mapping
    fields_dict = {'SF_AccountName': {'on': 'INP_Company',
                                     'index': 'sortedneighbourhood'},
                  'SF_CleanName': {'on': 'INP_Outreach_Account_Natural_Name'}}

  return fields_dict

# @title Geo Functions

def fix_country(df):
    country_map = {
        'The Bahamas': 'Bahamas',
        'Virgin Islands (British)': 'British Virgin Islands',
        'Cape Verde': 'Cabo Verde',
        'Democratic Republic of the Congo': 'Congo, Democratic Republic of the',
        'Czech Republic': 'Czechia',
        'Fiji Islands': 'Fiji',
        'Aland Islands': 'Åland Islands',
        'Gambia The': 'Gambia',
        'Palestinian Territory Occupied': 'Palestine',
        'Vatican City State (Holy See)': 'Holy See (Vatican City State)',
        'Hong Kong S.A.R.': 'Hong Kong',
        'Cote D\'Ivoire (Ivory Coast)': 'Côte d\'Ivoire',
        'North Korea': 'Korea, Democratic People\'s Republic of',
        'South Korea': 'Korea, Republic of',
        'Macau S.A.R.': 'Macao',
        'Curaçao': 'Curaçao',
        'Sint Maarten (Dutch part)': 'Sint Maarten',
        'Bonaire, Sint Eustatius and Saba': 'Bonaire, Sint Eustatius, and Saba',
        'Micronesia': 'Micronesia, Federated States of',
        'Pitcairn Island': 'Pitcairn Islands',
        'East Timor': 'Timor-Leste',
        'Russia': 'Russian Federation',
        'Saint-Barthelemy': 'Saint Barthelemy',
        'Saint Helena': 'Saint Helena, Ascension and Tristan da Cunha',
        'Saint-Martin (French part)': 'Saint Martin',
        'Svalbard And Jan Mayen Islands': 'Svalbard and Jan Mayen',
        'Swaziland': 'Eswatini',
        'Guernsey and Alderney': 'Guernsey',
        'Man (Isle of)': 'Isle of Man',
        'United States': 'United States of America',
        'Virgin Islands (US)': 'U. S. Virgin Islands',
        'Wallis And Futuna Islands': 'Wallis and Futuna'
    }

    df['CountryName'] = df['Country_Name'].replace(country_map)

    return df

def fix_states(df):
    ls = ['District', 'Municipality', 'Province', 'Region', 'County', 'Prefecture', 'Department', 'Oblast', 'Atoll', 'City of', 'London Borough of ']
    df['State_Name'] = df['State_Name'].replace(ls, '', regex=True)
    return df

def standardize_countries(df, df_country, country_col):
    # Create a set of valid country names
    ls_country = set(df_country['CountryName'])

    # Create a dictionary to map country names and codes to standardized values
    country_map = {}
    for col_name in ['CountryName', 'Country_Name', 'Country_ISO3', 'Country_ISO2']:
        country_map.update(dict(zip(df_country[col_name].str.lower(), df_country['CountryName'])))

    # Apply the mapping to the country column
    df['StandardizedCountry'] = df[country_col].str.lower().map(country_map)

    # Check if the standardized country is valid
    df['ValidCountry'] = df[country_col].isin(ls_country).astype(int)
    df['ValidStandardizedCountry'] = df['StandardizedCountry'].isin(ls_country).astype(int)

    return df

def standardize_states(df, df_states, state_col):
    # Create a set of valid state names
    ls_state = set(df_states['State_Name'])

    # Create a dictionary to map state names and codes to standardized values
    state_map = {}
    for col_name in ['State_Name', 'State_Code']:
        state_map.update(dict(zip(df_states[col_name].str.lower(), df_states['State_Name'])))

    # Apply the mapping to the state column
    df['StandardizedState'] = df[state_col].str.lower().map(state_map)

    # Check if the standardized state is valid
    df['ValidState'] = df[state_col].isin(ls_state).astype(int)
    df['ValidStandardizedState'] = df['StandardizedState'].isin(ls_state).astype(int)

    return df


def standardize_cities(df, df_city, city_col):
    # Create a set of valid state names
    ls_city = set(df_city['City_Name'])

    # Create a dictionary to map state names and codes to standardized values
    city_map = {}
    for col_name in ['City_Name']:
        city_map.update(dict(zip(df_city[col_name].str.lower(), df_city['City_Name'])))

    # Apply the mapping to the state column
    df['StandardizedCity'] = df[city_col].str.lower().map(city_map)

    # Check if the standardized state is valid
    df['ValidStandardizedCity'] = df['StandardizedCity'].isin(ls_city).astype(int)

    return df

# @title DNB_ZI_Match

def revenue_source(row):
    lower_cutoff = -10
    higher_cutoff = 10
    zero_revenue = row['SF_AnnualRevenue'] == 0

    conditions = {

            'DNB_Exact_Match': row['SF_AnnualRevenue'] == row['DNB_SalesVolumeUSDollars'],
            'ZI_Exact_Match': row['SF_AnnualRevenue'] == row['ZI_Revenue_Deviation'],

            f'DNB_Under_{higher_cutoff}': (
                row['DNB_SalesVolumeUSDollars'] > 0 and
                lower_cutoff <= row['DNB_Deviation(%)'] <= higher_cutoff and
                (abs(row['ZI_AnnualRevenue']) == 0 or abs(row['DNB_Deviation(%)']) <= abs(row['ZI_Deviation(%)']))
            ),
            f'ZI_Under_{higher_cutoff}': (
                row['ZI_AnnualRevenue'] > 0 and
                lower_cutoff <= row['ZI_Deviation(%)'] <= higher_cutoff and
                (abs(row['DNB_SalesVolumeUSDollars']) == 0 or abs(row['DNB_Deviation(%)']) >= abs(row['ZI_Deviation(%)']))
            ),

            'Update_Zero_from_DNB': row['SF_AnnualRevenue'] == 0 and row['DNB_SalesVolumeUSDollars'] > 0,
            'Update_Zero_from_ZI': row['SF_AnnualRevenue'] == 0 and row['ZI_AnnualRevenue'] > 0,

            'Needs_Review': True
        }

    for label, condition in conditions.items():
        if condition:
            return label

# @title Plotly

def create_bar_plot(df, title):
    """
    Create a bar plot using Plotly.

    Parameters:
        df (DataFrame): DataFrame containing the data for the plot.
        title (str): Title of the plot.

    Returns:
        None
    """
    # Transpose the DataFrame for easier plotting
    df = df.transpose()

    # Create the traces for each stage
    traces = []
    for stage in df.index:
        trace = go.Bar(x=df.columns, y=df.loc[stage], name=stage)
        traces.append(trace)

    # Create the layout
    layout = go.Layout(
        title=title,
        xaxis=dict(title='Category'),
        yaxis=dict(title='Values')
    )

    # Create the figure
    fig = go.Figure(data=traces, layout=layout)

    # Show the plot
    pio.show(fig)

# @title PDF Tables

def extract_tables_from_pdf(file_path):
    dfs = []
    with pdfplumber.open(file_path) as pdf:
        for i, page in enumerate(pdf.pages):
            table = page.extract_table()
            if table:
                try:
                    if i == 0:  # first page, use the first row as headers
                        df_page = pd.DataFrame(table[1:], columns=table[0])
                    else:  # other pages, use the headers from the first page
                        df_page = pd.DataFrame(table, columns=dfs[0].columns)
                    dfs.append(df_page)
                    # print(f"Page {i}, {df_page.shape} processed.")
                except ValueError:
                    print(f"Page {i} has a different table structure and was skipped.")
                    break
        df = pd.concat(dfs, ignore_index=True)
    return df